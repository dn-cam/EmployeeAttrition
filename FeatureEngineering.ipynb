{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import plotly.express as px\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import RFE\n",
    "from yellowbrick.model_selection import RFECV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from math import sqrt \n",
    "from sklearn import metrics\n",
    "import os\n",
    "import pastry\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "data = pd.read_csv('data/WA_Fn-UseC_-HR-Employee-Attrition.csv')\n",
    "data['attrition_binary'] = pd.Series(np.where(data.Attrition.values == 'Yes', 1, 0),data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cate_var = [list(data.columns)[i] for i in [2,4,6,7,10,11,13,14,15,16,17,21,22,25,27,30]] #store all categorical features' name in a list\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop(columns=['Attrition','attrition_binary','EmployeeNumber','Education','Gender','Over18']),data['attrition_binary'], test_size=0.25, random_state=42)\n",
    "cate_var_index_data = [1,3,5,7,9,10,11,12,13,17,19,20,22,25] # list containing indexes of all categorical variables in X_train\n",
    "cate_var = [list(data.drop(columns=['Attrition','attrition_binary','EmployeeNumber','Education','Gender','Over18']).columns)[i] for i in cate_var_index_data] # dropping statistically insignificant features\n",
    "sm = SMOTENC(sampling_strategy=1.0,random_state=42, categorical_features = cate_var_index_data,k_neighbors=5 )\n",
    "X_res, y_res = sm.fit_resample(X_train, y_train) #fitting SMOTE on training set\n",
    "\n",
    "encoding_drop_col_list = [] # define the list containing the levels to be dropped after one hot encoding\n",
    "for i in cate_var:\n",
    "    temp_df = pd.get_dummies(X_train[i], prefix=i)\n",
    "    encoding_drop_col_list.append(list(temp_df.columns)[0])\n",
    "\n",
    "for i in cate_var:  # one hot encoding for training set without SMOTE\n",
    "    temp_df = pd.get_dummies(X_res[i], prefix=i)\n",
    "    X_res_temp =X_res.join(temp_df)\n",
    "    X_res = X_res_temp\n",
    "X_res.drop(columns = cate_var+encoding_drop_col_list,inplace=True)\n",
    "\n",
    "for i in cate_var: # one hot encoding for training set with SMOTE\n",
    "    temp_df = pd.get_dummies(X_test[i], prefix=i)\n",
    "    X_test_temp =X_test.join(temp_df)\n",
    "    X_test = X_test_temp\n",
    "X_test.drop(columns = cate_var+encoding_drop_col_list,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['BusinessTravel', 'JobInvolvement'],\n",
       " ['Department', 'JobLevel'],\n",
       " ['Department', 'MaritalStatus'],\n",
       " ['EducationField', 'MaritalStatus'],\n",
       " ['EnvironmentSatisfaction', 'JobInvolvement'],\n",
       " ['JobInvolvement', 'JobLevel'],\n",
       " ['JobRole', 'MaritalStatus'],\n",
       " ['JobRole', 'RelationshipSatisfaction']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find all first order interactions\n",
    "def find_second_order_dependency(df:pd.DataFrame(),cate_var:list)->list:\n",
    "    dependency_list = []\n",
    "    for i in range(len(cate_var)):\n",
    "        for j in range(i+1,len(cate_var)):\n",
    "            i_col = cate_var[i]\n",
    "            j_col = cate_var[j]\n",
    "            chi2_result = stats.chi2_contingency(pd.DataFrame.to_numpy(pd.pivot_table(df, values='attrition_binary', index=[i_col],columns=[j_col], aggfunc=np.count_nonzero)))\n",
    "            if chi2_result[1]<0.05:\n",
    "                dependency_list.append([i_col,j_col])\n",
    "    return dependency_list\n",
    "find_second_order_dependency(data,cate_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering by adding interaction terms\n",
    "X_res_interaction = X_res.copy()\n",
    "X_test_interaction = X_test.copy()\n",
    "for i in range(len(find_second_order_dependency(data,cate_var))):\n",
    "    col1 = find_second_order_dependency(data,cate_var)[i][0]\n",
    "    col2 = find_second_order_dependency(data,cate_var)[i][1]\n",
    "    col1_levels = [j for j in list(X_res.columns) if col1 in j]\n",
    "    col2_levels = [k for k in list(X_res.columns) if col2 in k]\n",
    "    col1_list = []\n",
    "    col2_list = []\n",
    "    for m in col1_levels:\n",
    "        col1_list = [p for p in X_res[m].to_list()]\n",
    "        col1_list_test = [p for p in X_test[m].to_list()]\n",
    "        for n in col2_levels:\n",
    "            col2_list =[q for q in X_res[n].to_list()]\n",
    "            col2_list_test =[q for q in X_test[n].to_list()]\n",
    "            multiplied_list = [col1_list[i]*col2_list[i] for i in range(len(col1_list))]\n",
    "            multiplied_list_test = [col1_list_test[i]*col2_list_test[i] for i in range(len(col1_list_test))]\n",
    "            temp_merged_name = m+'_'+n\n",
    "            X_res_interaction[temp_merged_name] = multiplied_list\n",
    "            X_test_interaction[temp_merged_name] = multiplied_list_test\n",
    "X_res = X_res_interaction\n",
    "X_test = X_test_interaction\n",
    "# output dataframes: X_res, y_res, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in original data:  36\n",
      "\n",
      "Number of features in new Data:  148\n"
     ]
    }
   ],
   "source": [
    "print('Number of features in original data: ', len(data.columns))\n",
    "print('\\nNumber of features in new Data: ', len(X_res.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=Classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(m.X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_res.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildANNModel(data):\n",
    "    cl = Sequential()\n",
    "    cl.add(Dense(20, activation='relu', kernel_initializer='random_normal',  input_shape=(data.shape[1],)))\n",
    "    cl.add(Dropout(0.1))\n",
    "    cl.add(Dense(10, activation='relu', kernel_initializer='random_normal',  input_shape=(data.shape[1],)))\n",
    "    cl.add(Dropout(0.1))\n",
    "    cl.add(Dense(5, activation='relu', kernel_initializer='random_normal',  input_shape=(data.shape[1],)))\n",
    "    cl.add(Dropout(0.1))\n",
    "    cl.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))\n",
    "    adm = Adam()\n",
    "    cl.compile(optimizer=adm, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return cl\n",
    "\n",
    "def trainANNModel(X_train, y_train, epochs=10, batch_size=10, validation_split=0.33, verbose=0):\n",
    "    #model = buildANNModel(X_train)\n",
    "    model.fit(X_train, y_train, validation_split=validation_split, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "def evaluateANNModel(model, testX,testy):\n",
    "    yhat_probs = model.predict(testX, verbose=0)\n",
    "    yhat_classes = model.predict_classes(testX, verbose=0)\n",
    "    yhat_probs = yhat_probs[:, 0]\n",
    "    yhat_classes = yhat_classes[:, 0]\n",
    "\n",
    "    accuracy = accuracy_score(testy, yhat_classes)\n",
    "    print('Accuracy: %f' % accuracy)\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = precision_score(testy, yhat_classes)\n",
    "    print('Precision: %f' % precision)\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = recall_score(testy, yhat_classes)\n",
    "    print('Recall: %f' % recall)\n",
    "    # f1: 2 tp / (2 tp + fp + fn)\n",
    "    f1 = f1_score(testy, yhat_classes)\n",
    "    print('F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.130435\n",
      "Precision: 0.130435\n",
      "Recall: 1.000000\n",
      "F1 score: 0.230769\n"
     ]
    }
   ],
   "source": [
    "model = buildANNModel(X_res)\n",
    "trainANNModel(X_res,y_res)\n",
    "evaluateANNModel(model, X_test,y_test)\n",
    "\n",
    "#ann.trainANNModel(X_res,y_res)\n",
    "#ann.evaluateANNModel(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel Degree = 3\n",
      "Accuracy:  0.5679347826086957\n",
      "ROC AUC Score:  0.5479166666666667\n"
     ]
    }
   ],
   "source": [
    "model = Classification()\n",
    "data = model.data\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import auc,classification_report, roc_curve, roc_auc_score\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_res, y_res)\n",
    "predicted = clf.predict(X_test)\n",
    "\n",
    "print('Kernel Degree = 3')\n",
    "# get the accuracy\n",
    "print('Accuracy: ',accuracy_score(y_test, predicted))\n",
    "print('ROC AUC Score: ', roc_auc_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel Degree = 3\n",
      "Accuracy:  0.42391304347826086\n",
      "ROC AUC Score:  0.509375\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import auc,classification_report, roc_curve, roc_auc_score\n",
    "\n",
    "clf = svm.SVC(kernel='linear', max_iter=100)\n",
    "clf.fit(X_res, y_res)\n",
    "predicted = clf.predict(X_test)\n",
    "\n",
    "print('Kernel Degree = 3')\n",
    "# get the accuracy\n",
    "print('Accuracy: ',accuracy_score(y_test, predicted))\n",
    "print('ROC AUC Score: ', roc_auc_score(y_test, predicted))\n",
    "\n",
    "accuracy = accuracy_score(testy, yhat_classes)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(testy, yhat_classes)\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(testy, yhat_classes)\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(testy, yhat_classes)\n",
    "print('F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
